{
  "units1": 256,
  "units2": 128,
  "activation": "relu",
  "alpha": 0.0008199081400024764,
  "learning_rate": 0.010411172632431374,
  "batch_size": 32,
  "epochs": 300
}